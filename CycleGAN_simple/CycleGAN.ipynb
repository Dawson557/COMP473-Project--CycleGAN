{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7wNjDKdQy35h"
   },
   "source": [
    "# Install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##If you're curious about the GPU you've been assigned\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TRm-USlsHgEV"
   },
   "outputs": [],
   "source": [
    "####Only run this the first time!###\n",
    "#Subsequently, run the next block with pull command\n",
    "!git clone https://github.com/charlesabouhaidar/COMP473-Project.git\n",
    "import os\n",
    "os.chdir('COMP473-Project')\n",
    "os.chdir('CycleGAN_simple')\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util.util import loss_to_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##This for sure doesn't work, you need to start the repo from the branch you're working on in the first place\n",
    "#!git checkout <branch_name>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Pt3igws3eiVp"
   },
   "outputs": [],
   "source": [
    "## Update colab to reflect changes made to repo, Honestly not sure this works though.\n",
    "!git pull"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8daqlgVhw29P"
   },
   "source": [
    "# Datasets\n",
    "Download one of the official datasets below (ordered by size, smallest to largest):\n",
    "<ul>\n",
    "    <li>grumpifycat</li>\n",
    "    <li>facades</li>\n",
    "    <li>apple2orange</li>\n",
    "    <li>horse2zebra</li>\n",
    "    <li>summer2winter_yosemite</li>\n",
    "    <li>cezanne2photo</li>\n",
    "    <li>ukiyoe2photo</li>\n",
    "    <li>monet2photo</li>\n",
    "    <li>vangogh2photo</li>\n",
    "    <li>iphone2dslr_flower</li>\n",
    "    <li>maps</li>\n",
    "   </ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vrdOettJxaCc"
   },
   "outputs": [],
   "source": [
    "!bash ./datasets/download_cyclegan_datasets.sh vangogh2photo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yFw1kDQBx3LN"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that all training done on colab will not be saved. After running the training script below make sure to save the files locally on your computer using the appropriate cell-blocks in \"Save Checkpoints Locally\", to keep the results.\n",
    "<ul>\n",
    "    <li>--dataroot: the directory for the data you downloaded above</li>\n",
    "    <li>--name: the name you'd like to give your experiment</li>\n",
    "    <li>--n_epochs: The number of epochs to train without weight decay</li>\n",
    "    <li>--n_epochs_decay: The number of epochs to train with weight decay, these occur after n_epochs</li>\n",
    "    <li>--continue_train: Set this boolean to load a model and continue training it.</li>\n",
    "    <li>There are more options for you to explore in options/base_options.py and option/train_options.py</li>\n",
    "   </ul>\n",
    "For experiment names I think it'd be helpful if we adopted the format dataset_name for example vangogh2photo_smalldiscriminator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0sp7TCT2x9dB"
   },
   "outputs": [],
   "source": [
    "!python train.py --dataroot ./datasets/summer2winter_yosemite --name <experiment_name> --n_epochs=40 --n_epochs_decay=40"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Checkpoints Locally\n",
    "Google Colab is definitely a bit annoying for this part. We have to manually save the checkpoints and then push them to the repo for the next time, or upload them directly to colab for continued training later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##saving images from checkpoints takes too long and risks a time-out. Getting just the latest models seems best\n",
    "from google.colab import files\n",
    "files.download(\"model_checkpoints/vangogh2photo_cycleweight1/latest_net_D_A.pth\")\n",
    "files.download(\"model_checkpoints/vangogh2photo_cycleweight1/latest_net_D_B.pth\")\n",
    "files.download(\"model_checkpoints/vangogh2photo_cycleweight1/latest_net_G_A.pth\")\n",
    "files.download(\"model_checkpoints/vangogh2photo_cycleweight1/latest_net_G_B.pth\")\n",
    "files.download(\"model_checkpoints/vangogh2photo_cycleweight1/loss_log.txt\")\n",
    "files.download(\"model_checkpoints/vangogh2photo_cycleweight1/train_opt.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##this saves all the other model checkpoints but no images\n",
    "from google.colab import files\n",
    "!rm -r model_checkpoints/vangogh2photo_cycleweight1/web\n",
    "!zip -r model_checkpoints/vangogh2photo_cycleweight1.zip model_checkpoints/vangogh2photo_cycleweight1\n",
    "files.download(\"model_checkpoints/vangogh2photo_cycleweight1.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9UkcaFZiyASl"
   },
   "source": [
    "# Testing / Inference\n",
    "Same deal as training. To save the test results run the \"Save Results Locally\" right after."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python test.py --dataroot datasets/summer2winter_yosemite --name <experiment_name> --no_dropout --model cycle_gan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Results Locally\n",
    "Just change the <experiment_name\\> to match the same name you used while training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!zip -r results/<experiment_name>.zip results/<experiment_name>/test_latest\n",
    "files.download(\"results/<experiment_name>_results.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:5px\">\n",
    "<hr style=\"height:5px\">\n",
    "### Linear Weight Increase on Cycle Loss (Short name: cycleweight)\n",
    "Defining feature of experiment:\n",
    "<p>Added a linear increase in the weight of the cycle consistency loss of main objective function. Where $k$ is the epoch number lambda was set accordingly. Extra hyperparameters for this experiment are $\\lambda_1$ and $c$.$$\\lambda_1 = 1.0, \\, \\lambda_k = \\min(1.0 + c*k, 20)$$</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px\">\n",
    "<h5> vangogh2photo_cycleweight Training Parameters</h5>\n",
    "<ul>\n",
    "    <li>Dataset: vangogh2photo (6287 images)</li>\n",
    "    <li>Number of Epochs: 80</li>\n",
    "    <li>Learning Rate: 0.0002</li>\n",
    "    <li>batch size: 1</li>\n",
    "    <li>image size: 128 x 128</li>\n",
    "    <li>discriminators: 2 layer PatchGAN with 70 x 70 window</li>\n",
    "    <li>generators: 6 block Residual Network </li>\n",
    "    <li>pool size: 50</li>\n",
    "    <li>$\\lambda_1$: 1</li>\n",
    "    <li>$c$: 0.1</li>\n",
    "</ul>\n",
    "<p>\n",
    "<i>***Training halted due to poor results at 80 epoch mark. Notice red artefacts in our result images below. We believe this is due to using too small a discriminator.</i></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python train.py --dataroot ./datasets/vangogh2photo --name vangogh2photo_cycleweight --n_epochs=20 --n_epochs_decay=0 --cycle_weight=0.9 --cycle_weight_inc=0.1 --save_latest_freq=125740 --save_epoch_freq=20 --print_freq=6287"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>vangogh2photo_cycleweight Training Losses</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_to_graph(\"vangogh2photo_cycleweight1\", \"report_resources/logs/vangogh2photo_cycleweight1_loss_log.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Inference Images</h5>\n",
    " <img src=\"report_resources/images/vangogh2photo_cycleweight1.png\" style=\"width:1000px;height:500px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px\">\n",
    "<h5> horse2zebra_cycleweight Training Parameters</h5>\n",
    "<ul>\n",
    "<li>Dataset: horse2zebra (1334 images)</li>\n",
    "    <li>Number of Epochs: 200, 100 without weight decay followed by 100 with linear weight decay to 0</li>\n",
    "<li>Learning Rate: 0.0002</li>\n",
    "<li>batch size: 1</li>\n",
    "<li>image size: 128 x 128</li>\n",
    "<li>discriminators: 3 layer PatchGAN with 70 x 70 window</li>\n",
    "<li>generators: 6 block Residual Network </li>\n",
    "<li>pool size: 50</li>\n",
    "     <li>$\\lambda_1$: 1</li>\n",
    "    <li>$c$: 0.1</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python train.py --dataroot ./datasets/horse2zebra --name horse2zebra_cycleweight2 --n_epochs=20 --n_epochs_decay=0 --cycle_weight=0.9 --cycle_weight_inc=0.1 --save_latest_freq=125740 --save_epoch_freq=20 --print_freq=6287"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5> horse2zebra_cycleweight Training Losses</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "loss_to_graph(\"horse2zebra_cycleweight2\", \"report_resources/logs/horse2zebra_cycleweight2_loss_log.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Inference Images</h5>\n",
    "Results from the paper are on the left with our results on the same images on the right. You can see that ours kept from changing the background colours as much and overall wasn't as zealous with adding strips to the images.<br>\n",
    "<table>\n",
    "    <tr><th style=\"text-align:center\">Original Photo</th><th style=\"text-align:center\">Results from Zhu et. al</th><th style=\"text-align:center\">Experiment Results</th></tr>\n",
    "    <tr><td><img src=\"report_resources/images/horse2zebra_cycleweight2_1org.png\" style=\"width:200px;height:200px\"/></td><td><img src=\"report_resources/images/horse2zebra_cycleweight2_1zhu.png\" style=\"width:200px;height:200px\"/></td><td><img src=\"report_resources/images/horse2zebra_cycleweight2_1ours.png\" style=\"width:200px;height:200px\"/></td></tr>\n",
    "    \n",
    "   <tr><td><img src=\"report_resources/images/horse2zebra_cycleweight2_2org.png\" style=\"width:200px;height:200px\"/></td><td><img src=\"report_resources/images/horse2zebra_cycleweight2_2zhu.png\" style=\"width:200px;height:200px\"/></td><td><img src=\"report_resources/images/horse2zebra_cycleweight2_2ours.png\" style=\"width:200px;height:200px\"/></td></tr>\n",
    "   \n",
    "    <tr><td><img src=\"report_resources/images/horse2zebra_cycleweight2_3org.png\" style=\"width:200px;height:200px\"/></td><td><img src=\"report_resources/images/horse2zebra_cycleweight2_3zhu.png\" style=\"width:200px;height:200px\"/></td><td><img src=\"report_resources/images/horse2zebra_cycleweight2_3ours.png\" style=\"width:200px;height:200px\"/></td></tr>\n",
    "    \n",
    "    <tr><td><img src=\"report_resources/images/horse2zebra_cycleweight2_4org.png\" style=\"width:200px;height:200px\"/></td><td><img src=\"report_resources/images/horse2zebra_cycleweight2_4zhu.png\" style=\"width:200px;height:200px\"/></td><td><img src=\"report_resources/images/horse2zebra_cycleweight2_4ours.png\" style=\"width:200px;height:200px\"/></td></tr>\n",
    "   </table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:5px\">\n",
    "<hr style=\"height:5px\">\n",
    "### Training Generators with Ensemble of Discriminators (Short Name: ensemble)\n",
    "Defining Feature of Experiment:\n",
    "<p>In this experiment we swapped out the discriminators for other pre-trained discriminators and concluded training with those. The idea being that generators were less likely to find \"tricks\" to fool the discriminators. Two added hyperparameters for this experiment. First is Number of Discriminators which is the number of discriminators used to train each generator. Second is Tuning Epochs which is the number of epochs extra epochs the generators trained with each extra discriminator.\n",
    "<hr style=\"height:2px\">\n",
    "<h5> horse2zebra_ensemble Training Parameters</h5>\n",
    "<ul>\n",
    "    <li>Dataset: horse2zebra (1334 images)</li>\n",
    "    <li>Number of Epochs: 220, 100 without weight decay followed by 100 with linear weight decay to 0. The rest are part of tuning epochs</li>\n",
    "    <li>Learning Rate: 0.0002</li>\n",
    "    <li>batch size: 1</li>\n",
    "    <li>image size: 128 x 128</li>\n",
    "    <li>discriminators: 3 layer PatchGAN with 70 x 70 window</li>\n",
    "    <li>generators: 6 block Residual Network </li>\n",
    "    <li>pool size: 50</li>\n",
    "    <li>Number of Discriminators: 2</li>\n",
    "    <li>Tuning Epochs: 20</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>horse2zebra_ensemble Training Losses</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_to_graph(\"horse2zebra_ensemble\", \"report_resources/logs/horse2zebra_ensemble_loss_log.txt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Inference Images</h5>\n",
    "<table>\n",
    "    <tr><th style=\"text-align:center\">Original Photo</th><th style=\"text-align:center\">Results from Zhu et. al</th><th style=\"text-align:center\">Experiment Results</th></tr>\n",
    "    <tr><td><img src=\"report_resources/images/horse2zebra_cycleweight2_1org.png\" style=\"width:200px;height:200px\"/></td><td><img src=\"report_resources/images/horse2zebra_cycleweight2_1zhu.png\" style=\"width:200px;height:200px\"/></td><td><img src=\"report_resources/images/horse2zebra_ensemble_1ours.png\" style=\"width:200px;height:200px\"/></td></tr>\n",
    "    \n",
    "   <tr><td><img src=\"report_resources/images/horse2zebra_cycleweight2_2org.png\" style=\"width:200px;height:200px\"/></td><td><img src=\"report_resources/images/horse2zebra_cycleweight2_2zhu.png\" style=\"width:200px;height:200px\"/></td><td><img src=\"report_resources/images/horse2zebra_ensemble_2ours.png\" style=\"width:200px;height:200px\"/></td></tr>\n",
    "   \n",
    "    <tr><td><img src=\"report_resources/images/horse2zebra_cycleweight2_3org.png\" style=\"width:200px;height:200px\"/></td><td><img src=\"report_resources/images/horse2zebra_cycleweight2_3zhu.png\" style=\"width:200px;height:200px\"/></td><td><img src=\"report_resources/images/horse2zebra_ensemble_3ours.png\" style=\"width:200px;height:200px\"/></td></tr>\n",
    "    \n",
    "    <tr><td><img src=\"report_resources/images/horse2zebra_cycleweight2_4org.png\" style=\"width:200px;height:200px\"/></td><td><img src=\"report_resources/images/horse2zebra_cycleweight2_4zhu.png\" style=\"width:200px;height:200px\"/></td><td><img src=\"report_resources/images/horse2zebra_ensemble_4ours.png\" style=\"width:200px;height:200px\"/></td></tr>\n",
    "   </table>"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "CycleGAN",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
